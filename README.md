[![Format and Lint](https://github.com/JaredBaileyDuke/prompt-enhancer-with-local-llm/actions/workflows/format_and_lint.yml/badge.svg)](https://github.com/JaredBaileyDuke/prompt-enhancer-with-local-llm/actions/workflows/format_and_lint.yml)
[![Python tests](https://github.com/JaredBaileyDuke/prompt-enhancer-with-local-llm/actions/workflows/python-tests_and_%20docker.yml/badge.svg)](https://github.com/JaredBaileyDuke/prompt-enhancer-with-local-llm/actions/workflows/python-tests_and_%20docker.yml)
[![Docker Build and Deploy](https://github.com/JaredBaileyDuke/prompt-enhancer-with-local-llm/actions/workflows/docker-build-deploy.yml/badge.svg)](https://github.com/JaredBaileyDuke/prompt-enhancer-with-local-llm/actions/workflows/docker-build-deploy.yml)

# Prompt Enhancer with a Local LLM
Creating an automatic prompt enhancer with a local LLM (using llamafile) for increased security and portability

## Why This Product?
- 100% local, 100% private, can run without an internet connection
- For Windows, Mac, and Linux machines
- Can be run on a CPU only machine
- Can detect and use your GPU, whether it is from Apple, NVIDIA, or AMD
- Open Source
- Free

## How To Videos
### Windows
### Mac
### Linux

